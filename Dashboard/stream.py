import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
from pyvis.network import Network
import altair as alt
import plotly.graph_objects as go
import os
from kiwipiepy import Kiwi
from textblob import TextBlob
import streamlit.components.v1 as components
import ast
from sqlalchemy import create_engine, text
from api import db_url
from gensim.models import Word2Vec

from transformers import BertTokenizerFast, BertForSequenceClassification
import torch

stopwords = ['ëŒ€í•˜', 'ë•Œë¬¸', 'ê²½ìš°', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë˜í•œ', 'ë˜ëŠ”', 'ë”°ë¼ì„œ', 
             'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ', 'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´', 
             'ê°€', 'ì—', 'ì™€', 'ê³¼', 'ì—ì„œ', 'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ì•Šë‹¤', 'ê°™ë‹¤', 'ë•Œë¬¸ì—',
            'ìœ„í•´', 'ëŒ€í•œ', 'ì—¬ëŸ¬', 'ëª¨ë“ ', 'ì–´ë–¤', 'í•˜ë©´', 'ê·¸ëŸ¬ë©´', 'ì—°í•©ë‰´ìŠ¤']

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
tokenizer = BertTokenizerFast.from_pretrained("sangrimlee/bert-base-multilingual-cased-nsmc")
model = BertForSequenceClassification.from_pretrained("sangrimlee/bert-base-multilingual-cased-nsmc")

# ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)  # ëª¨ë¸ì„ GPUë¡œ ì´ë™

@st.cache_data
def analyze_sentiment(text):
    # ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    inputs = {key: value.to(device) for key, value in inputs.items()}  # ì…ë ¥ ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™

    # ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ (ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits

    # ê°ì • ë¶„ë¥˜ ê²°ê³¼ ë„ì¶œ
    predicted_class = torch.argmax(logits, dim=1).item()
    if predicted_class == 0:
        return 'ë¶€ì •'
    else:
        return 'ê¸ì •'


@st.cache_data
def create_wordcloud(text):
    # Use font_path to correctly display Korean text, words are horizontal only
    wordcloud = WordCloud(width=1500, height=1200, background_color='white', font_path='malgun.ttf', prefer_horizontal=1.0).generate(text)
    fig, ax = plt.subplots()
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    return fig


def find_similar_words(word, model, topn=3):
    try:
        similar_words = model.wv.most_similar(word, topn=topn)
        return [w[0] for w in similar_words]
    except KeyError:
        return []

@st.cache_data
def train_word2vec_model(sentences):
    # Train a Word2Vec model
    model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)
    return model

def visualize_main_word_network(category, top_keywords, w2v_model):
    net = Network(notebook=False, 
                height='60rem', 
                width='100%', 
                bgcolor='#ffffff', 
                font_color='black',
                cdn_resources="remote",
                layout=True,
                neighborhood_highlight=True)
    
    # Add main node for the category
    net.add_node(category, label=category, size=30, color='red')
    
    # Add nodes for top keywords and connect them to the category node
    for keyword in top_keywords:
        net.add_node(keyword, label=keyword, size=20, color='lightblue', shape='circle')
        net.add_edge(category, keyword, color='gray')
        
        # Find related words for each keyword
        similar_words = find_similar_words(keyword, w2v_model, topn=3)
        for similar_word in similar_words:
            net.add_node(similar_word, label=similar_word, size=15, color='lightgreen', shape='circle')
            net.add_edge(keyword, similar_word, color='lightgray')
    
    # Ensure the output directory exists
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_path = os.path.join(output_dir, f'{category}_word_network.html')
    net.write_html(output_path)
    with open(output_path, 'r', encoding='utf-8') as HtmlFile:
        return HtmlFile.read()


def visualize_expanded_word_network(main_word, w2v_model):
    net = Network(notebook=False, height='600px', width='100%', bgcolor='#ffffff', font_color='black', layout=True)
    
    # Add main node for the selected word
    net.add_node(main_word, label=main_word, size=30, color='red', physics=False)
    
    # Find similar words and add to the network
    try:
        similar_words = w2v_model.wv.most_similar(main_word, topn=10)
        for similar_word, similarity in similar_words:
            net.add_node(similar_word, label=similar_word, size=20, color='lightgreen', physics=False)
            net.add_edge(main_word, similar_word, value=similarity, color='gray', physics=False)
            
            # Find similar words of similar words (2nd level)
            similar_words_level_2 = w2v_model.wv.most_similar(similar_word, topn=5)
            for sub_word, sub_similarity in similar_words_level_2:
                net.add_node(sub_word, label=sub_word, size=10, color='lightyellow', physics=False)
                net.add_edge(similar_word, sub_word, value=sub_similarity, color='lightgray', physics=False)
    except KeyError:
        # Skip if the keyword is not in the vocabulary
        pass
    
    # Ensure the output directory exists
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_path = os.path.join(output_dir, f'{main_word}_expanded_network.html')
    net.write_html(output_path)
    with open(output_path, 'r', encoding='utf-8') as HtmlFile:
        return HtmlFile.read()

# Load or create dataframe
@st.cache_data
def data_load(target_date, word_like = None):
    # ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ ìƒì„±
    engine = create_engine(db_url)
    
    # SQL ì¿¼ë¦¬ ìƒì„±
    if word_like:
        sql = f'''
        SELECT * 
        FROM social_data 
        WHERE url IS NOT NULL 
        AND title like '%{word_like}%'
        ORDER BY seq DESC
        LIMIT 10
        ''' 
    else:
        sql = f'''
        SELECT * 
        FROM social_data 
        WHERE url IS NOT NULL 
        AND DATE(date) = '{target_date.strftime('%Y-%m-%d')}'
        '''

    sql = text(sql)
    
    # ë°ì´í„° ë¡œë“œ ë° ë‚ ì§œ í˜•ì‹ ë³€í™˜
    with engine.connect() as conn:
        df = pd.read_sql(sql, conn)
    df['date'] = pd.to_datetime(df['date'], errors='coerce')

    # contentì™€ sentencesì˜ '-' ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
    df['content'] = df['content'].replace('-', "['-']")
    df['sentences'] = df['sentences'].replace('-', "['-']")

    # contentì™€ sentencesë¥¼ ë¦¬ìŠ¤íŠ¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    df['content'] = df['content'].apply(ast.literal_eval)
    df['sentences'] = df['sentences'].apply(ast.literal_eval)
    
    return df

import streamlit as st
import pandas as pd
import altair as alt
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from kiwipiepy import Kiwi
import streamlit.components.v1 as components
import os

import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from prophet import Prophet
import streamlit as st


# ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_stock_data(stock_symbol, period="5y", interval="1d"):
    stock = yf.Ticker(stock_symbol)
    return stock.history(period=period, interval=interval)

# Streamlit ì£¼ê°€ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ
def stock_prediction_dashboard():
    st.title('ì£¼ê°€ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ ğŸ“ˆ')
    st.write("ì´ í˜ì´ì§€ì—ì„œëŠ” ì£¼ì‹ ì½”ë“œì™€ ì˜ˆì¸¡ ê¸°ê°„ì„ ì…ë ¥í•˜ì—¬ í•´ë‹¹ ì£¼ì‹ì˜ í–¥í›„ ê°€ê²©ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")

    # ì£¼ì‹ ì½”ë“œ ì…ë ¥ í¼
    with st.form(key='stock_form'):
        st.markdown("### ì£¼ì‹ ì½”ë“œì™€ ì˜ˆì¸¡ ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš”:")
        stock_symbol = st.text_input('ì£¼ì‹ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: TSLA, AAPL ë“±)', value='TSLA')
        prediction_period = st.number_input('ì˜ˆì¸¡í•  ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš” (ì¼ ë‹¨ìœ„, ìµœëŒ€ 30ì¼)', min_value=1, max_value=30, value=10)
        related_word = st.text_input('í•´ë‹¹ ì£¼ì‹ê³¼ ì—°ê´€ì´ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: íŠ¸ëŸ¼í”„, í…ŒìŠ¬ë¼ ë“±, ì„ íƒ)')
        submit_button = st.form_submit_button(label='ì˜ˆì¸¡í•˜ê¸°')

    if submit_button:
        try:
            # ì£¼ì‹ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ìµœê·¼ 5ë…„, 1ì¼ ë‹¨ìœ„)
            data = get_stock_data(stock_symbol)

            # ë°ì´í„°ê°€ ë¹„ì–´ ìˆì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
            if data.empty:
                st.error("ì£¼ì‹ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì£¼ì‹ ì½”ë“œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return

            # Prophetì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ í˜•ì‹ ë³€í™˜ (íƒ€ì„ì¡´ ì œê±°)
            prophet_data = data.reset_index()[['Date', 'Close']]
            prophet_data['Date'] = prophet_data['Date'].dt.tz_localize(None)  # íƒ€ì„ì¡´ ì œê±°
            prophet_data.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)

            # Prophet ëª¨ë¸ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
            model = Prophet(
                yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=False,
                changepoint_prior_scale=0.05  # ë³€ë™ì  ë¯¼ê°ë„ ì¡°ì •
            )
            model.add_seasonality(name='monthly', period=30.5, fourier_order=5)  # ì›”ë³„ ê³„ì ˆì„± ì¶”ê°€
            model.fit(prophet_data)

            # ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            future = model.make_future_dataframe(periods=prediction_period)
            forecast = model.predict(future)

            # ìŒìˆ˜ ì˜ˆì¸¡ ê°’ì„ 0ìœ¼ë¡œ ë³€í™˜
            forecast['yhat'] = forecast['yhat'].apply(lambda x: max(x, 0))

            # ìº”ë“¤ ì°¨íŠ¸ ë° ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
            st.subheader('ìº”ë“¤ ì°¨íŠ¸ ë° ì˜ˆì¸¡ ê²°ê³¼')
            fig = go.Figure()

            # ìº”ë“¤ì°¨íŠ¸ ì¶”ê°€
            fig.add_trace(go.Candlestick(
                x=data.index,
                open=data['Open'],
                high=data['High'],
                low=data['Low'],
                close=data['Close'],
                name='ìº”ë“¤ ì°¨íŠ¸'
            ))

            # ì˜ˆì¸¡ ë¼ì¸ ì¶”ê°€
            fig.add_trace(go.Scatter(
                x=forecast['ds'],
                y=forecast['yhat'],
                mode='lines',
                name='ì˜ˆì¸¡ ê°€ê²©',
                line=dict(color='red', width=2)
            ))

            # ì£¼ì‹ ì‹¤ì‹œê°„ ê°€ê²© ì—…ë°ì´íŠ¸ ë° ì°¨ì´ í‘œì‹œ
            current_price = data['Close'][-1]
            predicted_price = forecast['yhat'].iloc[-1]
            price_difference = predicted_price - current_price
            price_color = 'red' if price_difference > 0 else 'blue'

            st.markdown(f"### {stock_symbol} ì‹¤ì‹œê°„ ê°€ê²©: ${current_price:.2f}")
            st.markdown(f"### ì˜ˆì¸¡ ê°€ê²©ê³¼ì˜ ì°¨ì´: <span style='color:{price_color};'>${price_difference:.2f}</span>", unsafe_allow_html=True)

            # ì°¨íŠ¸ ë ˆì´ì•„ì›ƒ ì„¤ì • ë° ì¶œë ¥
            fig.update_layout(
                title=f'{stock_symbol} ì£¼ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼',
                xaxis_title='ë‚ ì§œ',
                yaxis_title='ê°€ê²©',
                xaxis_rangeslider_visible=False
            )

            st.plotly_chart(fig)

            # ê´€ë ¨ ë‰´ìŠ¤ ì¶œë ¥
            col1, col2 = st.columns([0.3, 0.7])
            with col1:
                # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ (ì˜ˆì¸¡í•œ ê¸°ê°„ë§Œ)
                st.subheader('ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°')
                future_predictions = forecast[['ds', 'yhat']].tail(prediction_period)
                future_predictions.columns = ['ë‚ ì§œ', 'ì˜ˆì¸¡ ê°€ê²©']
                st.dataframe(future_predictions)


            with col2:
                if len(related_word) > 1:
                    df_related = data_load(None, related_word)

                    st.markdown(f"### ğŸŒ {related_word} ê´€ë ¨ ë‰´ìŠ¤")
                    category_news = df_related.tail(10)
                    for i, (index, row) in enumerate(category_news.iterrows()):
                        st.markdown(f"<div style='margin-bottom: 10px;'><strong>{i + 1}. <a href='{row['url']}' target='_blank'>{row['title']}</a></strong> ğŸŒ {row['publisher']}</div>", unsafe_allow_html=True)
                else:
                    st.subheader('ğŸ—… ì£¼ìš” ë‰´ìŠ¤')

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.write('ì—°ê´€ì–´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')

def daily_news_dashboard():
    # í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
    # st.set_page_config(layout='wide', page_title='ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ëŒ€ì‹œë³´ë“œ', page_icon='ğŸ“Š')
    st.title('ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ëŒ€ì‹œë³´ë“œ ğŸ“Š')

    # ì‚¬ì´ë“œë°” ë° í˜ì´ì§€ ì œëª©
    st.sidebar.title('ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸')
    st.sidebar.subheader("ë°ì´í„° ì„ íƒ")
    selected_date = st.sidebar.date_input('ë‚ ì§œ ì„ íƒ', pd.Timestamp('today'))

    # ì‚¬ì´ë“œë°” - íŠ¹ì • ë‹¨ì–´ í•„í„°ë§ ê¸°ëŠ¥
    st.sidebar.subheader("ğŸ” íŠ¹ì • ë‹¨ì–´ë¡œ ê¸°ì‚¬ í•„í„°ë§")
    filter_keywords = st.sidebar.text_area("ê²€ìƒ‰í•  ë‹¨ì–´ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„):")
    filter_keywords = [word.strip() for word in filter_keywords.split(',') if word.strip()]  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    reset_filter = st.sidebar.button("ğŸ”„ í•„í„° ì´ˆê¸°í™”")

    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    if selected_date:
        df = data_load(selected_date)
        if df.empty:
            st.warning("ì„ íƒí•œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # í•„í„° ì ìš©
            if filter_keywords:
                df = df[df['content'].apply(lambda x: any(keyword in ' '.join(x) for keyword in filter_keywords))]

            if reset_filter:
                filter_keywords = []  # í•„í„° ì´ˆê¸°í™”

            st.title(f"ğŸ“ {selected_date.strftime('%Yë…„ %mì›” %dì¼')} ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸")
            # íƒ­ êµ¬ì¡°ë¡œ ë‰´ìŠ¤ ì„¸ë¶€ ì •ë³´ í‘œì‹œ (íƒ­ì„ ìƒë‹¨ì— ë°°ì¹˜)
            tab_labels = ['ë©”ì¸', 'ì •ì¹˜', 'ê²½ì œ', 'ì‚¬íšŒ', 'ìƒí™œ/ë¬¸í™”', 'IT/ê³¼í•™']
            tabs = st.tabs(tab_labels)

            # ì‚¬ì´ë“œë°” - ì‹¤ì‹œê°„ WORDCOUNT TOP 10 ë‹¨ì–´
            st.sidebar.subheader("ğŸ”¥ ì‹¤ì‹œê°„ ì¸ê¸° ë‹¨ì–´ TOP 10")
            all_tokens = []
            kiwi = Kiwi()
            ##################################################################################################################################
            for sublist in df['sentences']:
                for sentence in sublist:
                    for word in sentence:
                        analyzed = kiwi.analyze(word)
                        if analyzed:
                            morphs = analyzed[0][0]
                            for token in morphs:
                                if token.tag.startswith('N') and len(token.form) > 1 and token.tag == "NNP":
                                    all_tokens.append(token.form)
            ##################################################################################################################################
            word_count = Counter(all_tokens)
            word_count_df = pd.DataFrame(word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
            for i, (index, row) in enumerate(word_count_df.iterrows()):
                st.sidebar.markdown(f"**{i + 1}. {row['Word']}**")

            # ë©”ì¸ í™”ë©´ ë ˆì´ì•„ì›ƒ - ì»¬ëŸ¼ ì‚¬ìš©ìœ¼ë¡œ ê°€ë…ì„± ê°œì„ 
            with tabs[0]:
                main_container = st.container()
                with main_container:
                    st.markdown("---")
                    summary_col, chart_col = st.columns([2, 1], gap="medium")

                    # ì£¼ìš” ë‰´ìŠ¤ ìš”ì•½ ì •ë³´ (ë¶„ì•¼ë³„ë¡œ 5ê°œì”©ë§Œ ë³´ì´ë„ë¡ ìˆ˜ì •)
                    with summary_col:
                        st.subheader('ğŸ“° ì£¼ìš” ë‰´ìŠ¤')
                        categories = df['category'].unique()
                        for category in categories:
                            st.markdown(f"### ğŸŒ {category} ë‰´ìŠ¤")
                            category_news = df[df['category'] == category].tail(5)
                            for i, (index, row) in enumerate(category_news.iterrows()):
                                st.markdown(f"<div style='margin-bottom: 10px;'><strong>{i + 1}. <a href='{row['url']}' target='_blank'>{row['title']}</a></strong> ğŸŒ {row['publisher']}</div>", unsafe_allow_html=True)

                    # ë¶„ì•¼ë³„ ë‰´ìŠ¤ ê°œìˆ˜ ë° ê¸/ë¶€ì • ë¹„ìœ¨ ì‹œê°í™”
                    with chart_col:
                        st.subheader('ğŸ“Š ë¶„ì•¼ë³„ ë‰´ìŠ¤ ê°œìˆ˜')
                        news_count_by_category = df['category'].value_counts()
                        news_count_df = pd.DataFrame({'Category': news_count_by_category.index, 'Count': news_count_by_category.values})
                        category_chart = alt.Chart(news_count_df).mark_bar(color='steelblue').encode(
                            x=alt.X('Count', sort='-y'),
                            y=alt.Y('Category', sort='-x', axis=alt.Axis(labelFontSize=12)),
                            tooltip=['Category', 'Count']
                        ).properties(height=300)
                        st.altair_chart(category_chart, use_container_width=True)

                        # ë¶„ì•¼ë³„ ê¸/ë¶€ì • ë¹„ìœ¨ ì‹œê°í™”
                        st.subheader('ğŸ“Š ë¶„ì•¼ë³„ ê¸ì •/ë¶€ì • ë¹„ìœ¨')
                        df['sentiment'] = df['content'].apply(lambda x: analyze_sentiment(' '.join(x)))
                        
                        ##################################################################################################################################
                        with engine.connect() as conn:
                            for i, row in df.iterrows():
                                # Assuming there's an identifier or column you can match on (e.g., 'id')
                                sql = text("UPDATE social_data SET sentiment = :sentiment WHERE seq = :seq")
                                conn.execute(sql, {"sentiment": row['sentiment'], "seq": row['seq']})
                            conn.commit()
                        ##################################################################################################################################

                        sentiment_category_df = df.groupby(['category', 'sentiment']).size().reset_index(name='count')
                        sentiment_chart = alt.Chart(sentiment_category_df).mark_bar().encode(
                            x=alt.X('count', title='Count'),
                            y=alt.Y('category', title='Category', sort='-x'),
                            color='sentiment',
                            tooltip=['category', 'sentiment', 'count']
                        ).properties(height=300)
                        st.altair_chart(sentiment_chart, use_container_width=True)

            # ì¹´í…Œê³ ë¦¬ë³„ íƒ­ êµ¬ì„±
            for idx, selected_category in enumerate(tab_labels[1:]):
                with tabs[idx + 1]:
                    filtered_data = df[df['category'] == selected_category]
                    if filtered_data.empty:
                        st.warning('í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
                        continue

                    st.subheader(f"ğŸ“ {selected_category} ë‰´ìŠ¤ ë¶„ì„")
                    sentences = [sentence for sublist in filtered_data['sentences'] for sentence in sublist]
                    w2v_model = train_word2vec_model(sentences)

                    # ì¹´í…Œê³ ë¦¬ ìƒì„¸ ë‰´ìŠ¤ ì‹œê°í™” ë ˆì´ì•„ì›ƒ
                    st.markdown("---")
                    st.subheader('ğŸ’­ ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´ ë° ë„¤íŠ¸ì›Œí¬ ë¶„ì„')
                    cloud_network_col1, cloud_network_col2 = st.columns([1, 1], gap="large")

                    # ì›Œë“œ í´ë¼ìš°ë“œ ë° ì£¼ìš” ë‹¨ì–´ ë¶„ì„
                    with cloud_network_col1:
                        st.subheader('ğŸ” ì›Œë“œ í´ë¼ìš°ë“œ')
                        ##################################################################################################################################
                        tokens = []
                        for sublist in filtered_data['sentences']:
                            for sentence in sublist:
                                for word in sentence:
                                    analyzed = kiwi.analyze(word)

                                    if analyzed:
                                        morphs = analyzed[0][0]

                                    for token in morphs:
                                        if token.tag.startswith('N') and len(token.form) > 1 and token.form not in stopwords:
                                            tokens.append(token.form)
                        all_text = ' '.join(tokens)
                        ##################################################################################################################################
                        wordcloud_fig = create_wordcloud(all_text)
                        st.pyplot(wordcloud_fig)

                        # ì£¼ìš” ë‹¨ì–´ ë¹ˆë„ í…Œì´ë¸”
                        word_count = Counter(tokens)
                        word_count_df = pd.DataFrame(word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
                        st.table(word_count_df)

                    # ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
                    with cloud_network_col2:
                        st.subheader('ğŸŒ ë‹¨ì–´ ë„¤íŠ¸ì›Œí¬')
                        for word in word_count_df['Word']:
                            with st.expander(f"ğŸ› ï¸ {word} ìœ ì‚¬ ë‹¨ì–´ ë„¤íŠ¸ì›Œí¬ ë³´ê¸°"):
                                expanded_network_html = visualize_expanded_word_network(word.replace('/', '_'), w2v_model)
                                components.html(expanded_network_html, height=500)

                    # ê¸ì •, ë¶€ì • í‰ê°€ ì‹œê°í™” ë° ë‰´ìŠ¤ ì˜ˆì‹œ
                    st.markdown("---")
                    st.subheader('ğŸ—³ï¸ ê¸ì •, ë¶€ì • í‰ê°€ ë¹„ìœ¨ ë° ë‰´ìŠ¤')
                    pos_neg_col1, pos_neg_col2 = st.columns([1, 1], gap="large")

                    # ê¸ì •, ë¶€ì • í‰ê°€ ì‹œê°í™”
                    with pos_neg_col1:
                        st.subheader('ğŸ“Š ê¸/ë¶€ì • ë¹„ìœ¨')
                        ##################################################################################################################################
                        sentiments = filtered_data['content'].apply(lambda x: analyze_sentiment(' '.join(x)))
                        filtered_data['sentiment'] = sentiments
                        ##################################################################################################################################
                        sentiment_counts = sentiments.value_counts().to_dict()
                        sentiment_df = pd.DataFrame(list(sentiment_counts.items()), columns=['Sentiment', 'Count'])
                        pie_chart = alt.Chart(sentiment_df).mark_arc(innerRadius=50).encode(
                            theta=alt.Theta('Count', stack=True),
                            color=alt.Color('Sentiment', scale=alt.Scale(scheme='category10')),
                            tooltip=['Sentiment', 'Count']
                        ).properties(height=300)
                        st.altair_chart(pie_chart, use_container_width=True)

                    # ê¸ì •, ë¶€ì • ë‰´ìŠ¤
                    with pos_neg_col2:
                        st.subheader('âœ… ê¸ì • ë‰´ìŠ¤ TOP 5')
                        positive_data = filtered_data[filtered_data['sentiment'] == 'ê¸ì •'].tail(5)
                        for i, (index, row) in enumerate(positive_data.iterrows()):
                            st.markdown(f"<div style='margin-bottom: 10px;'><strong>{i + 1}. <a href='{row['url']}' target='_blank'>{row['title']}</a></strong> ğŸŒ {row['publisher']}</div>", unsafe_allow_html=True)

                        st.subheader('âŒ ë¶€ì • ë‰´ìŠ¤ TOP 5')
                        negative_data = filtered_data[filtered_data['sentiment'] == 'ë¶€ì •'].tail(5)
                        for i, (index, row) in enumerate(negative_data.iterrows()):
                            st.markdown(f"<div style='margin-bottom: 10px;'><strong>{i + 1}. <a href='{row['url']}' target='_blank'>{row['title']}</a></strong> ğŸŒ {row['publisher']}</div>", unsafe_allow_html=True)

                    # ê¸ì •, ë¶€ì • ë‰´ìŠ¤ì˜ ì£¼ìš” ë‹¨ì–´ ë¶„ì„
                    st.markdown("---")
                    st.subheader('ğŸ’¬ ê¸ì • ë° ë¶€ì • ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´')
                    pos_neg_word_col1, pos_neg_word_col2 = st.columns([1, 1], gap="large")

                    with pos_neg_word_col1:
                        st.subheader('ğŸ’¬ ê¸ì • ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´')
                        ##################################################################################################################################
                        positive_tokens = []
                        for sublist in positive_data['sentences']:
                            for sentence in sublist:
                                for word in sentence:
                                    analyzed = kiwi.analyze(word)
                                    if analyzed:
                                        morphs = analyzed[0][0]
                                        for token in morphs:
                                            if token.tag.startswith('N') and len(token.form) > 1 and token.form not in stopwords:
                                                positive_tokens.append(token.form)
                        ##################################################################################################################################
                        positive_word_count = Counter(positive_tokens)
                        positive_word_count_df = pd.DataFrame(positive_word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
                        st.table(positive_word_count_df)

                    with pos_neg_word_col2:
                        st.subheader('ğŸ’¬ ë¶€ì • ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´')
                        ##################################################################################################################################
                        negative_tokens = []
                        for sublist in negative_data['sentences']:
                            for sentence in sublist:
                                for word in sentence:
                                    analyzed = kiwi.analyze(word)
                                    if analyzed:
                                        morphs = analyzed[0][0]
                                        for token in morphs:
                                            if token.tag.startswith('N') and len(token.form) > 1 and token.form not in stopwords:
                                                negative_tokens.append(token.form)
                        ##################################################################################################################################
                        negative_word_count = Counter(negative_tokens)
                        negative_word_count_df = pd.DataFrame(negative_word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
                        st.table(negative_word_count_df)

                    st.markdown("---")
                    st.subheader(f'ğŸ›ˆ {selected_category} ì¤‘ì‹¬ ì›Œë“œ ë„¤íŠ¸ì›Œí¬')
                    if len(tokens) > 1:
                        top_keywords = word_count_df['Word'].tolist()
                        word_network_html = visualize_main_word_network(selected_category.replace('/', '_'), top_keywords, w2v_model)
                        components.html(word_network_html, height=500)
                    else:
                        st.warning('ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')

# ë©”ì¸ í•¨ìˆ˜
def main_dashboard():
    query_params = st.query_params
    page = query_params.get('page', 'main')

    if page == 'main':
        st.markdown(
            """
            <style>
            @import url('https://fonts.googleapis.com/css?family=Lato:100,300,400');
            * {
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }
            .button-container-2 {
                position: relative;
                width: 48%;
                height: 100px;
                margin: 2%;
                overflow: hidden;
                border: 1px solid #000;
                font-family: 'Lato', sans-serif;
                font-weight: 300;
                transition: 0.5s;
                letter-spacing: 1px;
                border-radius: 8px;
            }

            .button-container-2 button {
                width: 100%;
                height: 100%;
                font-family: 'Lato', sans-serif;
                font-weight: 300;
                font-size: 20px;
                letter-spacing: 1px;
                font-weight: bold;
                background: #000;
                -webkit-mask: url('https://raw.githubusercontent.com/robin-dela/css-mask-animation/master/img/urban-sprite.png');
                mask: url('https://raw.githubusercontent.com/robin-dela/css-mask-animation/master/img/urban-sprite.png');
                -webkit-mask-size: 3000% 100%;
                mask-size: 3000% 100%;
                border: none;
                color: #fff;
                cursor: pointer;
                -webkit-animation: ani2 0.7s steps(29) forwards;
                animation: ani2 0.7s steps(29) forwards;
            }

            .button-container-2 button:hover {
                -webkit-animation: ani 0.7s steps(29) forwards;
                animation: ani 0.7s steps(29) forwards;
            }

            .mas {
                position: absolute;
                color: #000;
                text-align: center;
                width: 100%;
                font-family: 'Lato', sans-serif;
                font-weight: 300;
                font-size: 20px;
                margin-top: 30px;
                overflow: hidden;
                font-weight: bold;
            }

            @-webkit-keyframes ani {
                from {
                    -webkit-mask-position: 0 0;
                    mask-position: 0 0;
                }
                to {
                    -webkit-mask-position: 100% 0;
                    mask-position: 100% 0;
                }
            }

            @keyframes ani {
                from {
                    -webkit-mask-position: 0 0;
                    mask-position: 0 0;
                }
                to {
                    -webkit-mask-position: 100% 0;
                    mask-position: 100% 0;
                }
            }

            @-webkit-keyframes ani2 {
                from {
                    -webkit-mask-position: 100% 0;
                    mask-position: 100% 0;
                }
                to {
                    -webkit-mask-position: 0 0;
                    mask-position: 0 0;
                }
            }

            @keyframes ani2 {
                from {
                    -webkit-mask-position: 100% 0;
                    mask-position: 100% 0;
                }
                to {
                    -webkit-mask-position: 0 0;
                    mask-position: 0 0;
                }
            }
            </style>
            <div style="display: flex; justify-content: space-between; align-items: center; width: 100%;">
                <div class="button-container-2">
                    <a href="?page=daily_news">
                    <span class="mas">ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸</span>
                    <button type="button" onclick="location.href='?page=daily_news'">ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸</button>
                    </a>
                </div>
                <div class="button-container-2">
                    <a href="?page=stock_prediction">
                    <span class="mas">ì£¼ê°€ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ</span>
                    <button type="button" onclick="location.href='?page=stock_prediction'">ì£¼ê°€ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ</button>
                    </a>
                </div>
            </div>
            """,
            unsafe_allow_html=True
        )

    if page == 'daily_news':
        st.markdown("<style>.button-container-2 { display: none; }</style>", unsafe_allow_html=True)
        st.session_state.logged_in = True
        daily_news_dashboard()
        st.sidebar.button("ğŸ  í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸°", on_click=lambda: st.query_params.update(page='main'))
        st.session_state.logged_in = True
    elif page == 'stock_prediction':
        st.markdown("<style>.button-container-2 { display: none; }</style>", unsafe_allow_html=True)
        stock_prediction_dashboard()
        st.sidebar.button("ğŸ  í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸°", on_click=lambda: st.query_params.update(page='main'))

import re

def login():
    st.set_page_config(layout='wide', page_title='ë¡œê·¸ì¸', page_icon='\U0001F512')

    # ë¡œê·¸ì¸ ì…ë ¥ í•„ë“œ ë° ìŠ¤íƒ€ì¼
    st.markdown(
        """
        <style>
        @import url('https://fonts.googleapis.com/css?family=Lato:100,300,400');
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background: #050801;
            font-family: 'Lato', sans-serif;
            font-weight: bold;
            color: #ffffff;
        }
        .login-container {
            text-align: center;
            width: 300px;
            padding: 40px;
            background: #333;
            border-radius: 10px;
            box-shadow: 0 0 20px #000;
        }
        input[type="text"], input[type="password"] {
            width: 100%;
            padding: 10px;
            margin: 10px 0;
            border-radius: 5px;
            border: none;
            font-size: 16px;
        }
        button {
            padding: 10px 20px;
            margin-top: 20px;
            background: #03e9f4;
            color: #050801;
            font-size: 16px;
            font-weight: bold;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: 0.3s;
        }
        button:hover {
            background: #0298b9;
        }
        </style>
        """, unsafe_allow_html=True
    )

    # ë¡œê·¸ì¸ í¼ ìƒì„±
    with st.form("login"):
        input_username = st.text_input("ì•„ì´ë””ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="input_username")
        input_password = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password", key="input_password")
        login_btn = st.form_submit_button("ë¡œê·¸ì¸")

    if login_btn:
        if input_username and input_password:
            if check_user(input_username, input_password):
                # ë¡œê·¸ì¸ ì„±ê³µ ì‹œ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° í˜ì´ì§€ ì „í™˜
                st.session_state.username = input_username
                st.session_state.logged_in = True
                st.rerun()
            else:
                # ë¡œê·¸ì¸ ì‹¤íŒ¨
                st.error("ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
        else:
            st.error("ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # íšŒì›ê°€ì… ë²„íŠ¼ ì¶”ê°€
    if st.button("íšŒì›ê°€ì…"):
        st.session_state.page = "register"
        st.rerun()


def register_user():
    st.subheader("íšŒì›ê°€ì…")
    with st.form('register'):
        register_username = st.text_input("ì•„ì´ë””ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="register_username")
        register_password = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password", key="register_password")
        register_email = st.text_input("ì´ë©”ì¼ì„ ì…ë ¥í•˜ì„¸ìš”", key="register_email")
        email_req = st.checkbox("ë©”ì¼ ìˆ˜ì‹  ì—¬ë¶€", key="email_req")

        submit = st.form_submit_button("íšŒì›ê°€ì…í•˜ê¸°")

    if submit:
        if not register_username or not register_password or not register_email:
            st.error("ëª¨ë“  í•„ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        # ì•„ì´ë”” ì¤‘ë³µ ì²´í¬
        if user_exists(register_username):
            st.error("ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ì•„ì´ë””ì…ë‹ˆë‹¤.")
            return

        # ë¹„ë°€ë²ˆí˜¸ ìœ íš¨ì„± ê²€ì‚¬ (5ì ì´ìƒ, ì˜ë¬¸ê³¼ ìˆ«ìê°€ ëª¨ë‘ í¬í•¨)
        if len(register_password) < 5 or not re.search("[a-zA-Z]", register_password) or not re.search("[0-9]", register_password):
            st.error("ë¹„ë°€ë²ˆí˜¸ëŠ” 5ì ì´ìƒì´ë©°, ì˜ë¬¸ê³¼ ìˆ«ìê°€ ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return

        # ì´ë©”ì¼ í˜•ì‹ ìœ íš¨ì„± ê²€ì‚¬
        if not re.match(r"[^@\s]+@[^@\s]+\.[^@\s]+", register_email):
            st.error("ì˜¬ë°”ë¥¸ ì´ë©”ì¼ í˜•ì‹ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        # íšŒì› ì •ë³´ ì €ì¥ (ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€)
        if save_user(register_username, register_password, register_email, email_req):
            st.success("íšŒì›ê°€ì…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë¡œê·¸ì¸í•´ ì£¼ì„¸ìš”.")
            st.session_state.page = "login"
            st.rerun()


def check_user(input_username, input_password):
    query = text("SELECT COUNT(*) FROM user_table WHERE id = :username AND password = :password")

    with engine.connect() as conn:
        result = conn.execute(query, {"username": input_username, "password": input_password}).scalar()
        return result > 0  # Returns True if the user exists, otherwise False


def user_exists(username):
    query = text("SELECT COUNT(*) FROM user_table WHERE id = :username")

    with engine.connect() as conn:
        result = conn.execute(query, {"username": username}).scalar()
        return result > 0


def save_user(username, password, email, email_req):
    email_req = 1 if email_req else 0
    try:
        with engine.connect() as conn:
            query = text("INSERT INTO user_table(id, password, email, email_req) VALUES(:username, :password, :email, :email_req)")
            conn.execute(query, {"username": username,
                                 "password": password,
                                 "email": email,
                                 "email_req": email_req})
            conn.commit()
        return True
    except Exception as e:
        st.error("íšŒì›ê°€ì…ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤. ì…ë ¥ì •ë³´ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print(e)
        return False


# Streamlit ì‹¤í–‰
if __name__ == '__main__':
    engine = create_engine(db_url)

    if 'logged_in' not in st.session_state:
        st.session_state.logged_in = False
        st.session_state.username = None
        st.session_state.page = "login"

    if st.session_state.page == "login":
        if not st.session_state.logged_in:
            login()
        else:
            st.success(f"ì•ˆë…•í•˜ì„¸ìš”, {st.session_state.username}ë‹˜!")
            # main_dashboard() í•¨ìˆ˜ í˜¸ì¶œ (ë©”ì¸ ëŒ€ì‹œë³´ë“œ í™”ë©´)
            main_dashboard()

    elif st.session_state.page == "register":
        register_user()