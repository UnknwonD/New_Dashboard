import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
from pyvis.network import Network
import altair as alt
import os
from kiwipiepy import Kiwi
from textblob import TextBlob
import streamlit.components.v1 as components
import ast
from sqlalchemy import create_engine, text
from api import db_url
from gensim.models import Word2Vec

from transformers import BertTokenizerFast, BertForSequenceClassification
import torch

stopwords = ['ëŒ€í•˜', 'ë•Œë¬¸', 'ê²½ìš°', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë˜í•œ', 'ë˜ëŠ”', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ', 'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì—', 'ì™€', 'ê³¼', 'ì—ì„œ', 'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ì•Šë‹¤', 'ê°™ë‹¤', 'ë•Œë¬¸ì—', 'ìœ„í•´', 'ëŒ€í•œ', 'ì—¬ëŸ¬', 'ëª¨ë“ ', 'ì–´ë–¤', 'í•˜ë©´', 'ê·¸ëŸ¬ë©´']

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
tokenizer = BertTokenizerFast.from_pretrained("sangrimlee/bert-base-multilingual-cased-nsmc")
model = BertForSequenceClassification.from_pretrained("sangrimlee/bert-base-multilingual-cased-nsmc")

# ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)  # ëª¨ë¸ì„ GPUë¡œ ì´ë™

@st.cache_data
def analyze_sentiment(text):
    # ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    inputs = {key: value.to(device) for key, value in inputs.items()}  # ì…ë ¥ ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™

    # ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ (ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits

    # ê°ì • ë¶„ë¥˜ ê²°ê³¼ ë„ì¶œ
    predicted_class = torch.argmax(logits, dim=1).item()
    if predicted_class == 0:
        return 'ë¶€ì •'
    else:
        return 'ê¸ì •'

# # Sentiment analysis function
# def analyze_sentiment(text):
#     analysis = TextBlob(text)
#     if analysis.sentiment.polarity > 0:
#         return 'ê¸ì •'
#     elif analysis.sentiment.polarity == 0:
#         return 'ì¤‘ë¦½'
#     else:
#         return 'ë¶€ì •'

@st.cache_data
def create_wordcloud(text):
    # Use font_path to correctly display Korean text, words are horizontal only
    wordcloud = WordCloud(width=1500, height=1200, background_color='white', font_path='malgun.ttf', prefer_horizontal=1.0).generate(text)
    fig, ax = plt.subplots()
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    return fig


def find_similar_words(word, model, topn=3):
    try:
        similar_words = model.wv.most_similar(word, topn=topn)
        return [w[0] for w in similar_words]
    except KeyError:
        return []

@st.cache_data
def train_word2vec_model(sentences):
    # Train a Word2Vec model
    model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)
    return model

def visualize_main_word_network(category, top_keywords, w2v_model):
    net = Network(notebook=False, 
                height='60rem', 
                width='100%', 
                bgcolor='#ffffff', 
                font_color='black',
                cdn_resources="remote",
                layout=True,
                neighborhood_highlight=True)
    
    # Add main node for the category
    net.add_node(category, label=category, size=30, color='red')
    
    # Add nodes for top keywords and connect them to the category node
    for keyword in top_keywords:
        net.add_node(keyword, label=keyword, size=20, color='lightblue', shape='circle')
        net.add_edge(category, keyword, color='gray')
        
        # Find related words for each keyword
        similar_words = find_similar_words(keyword, w2v_model, topn=3)
        for similar_word in similar_words:
            net.add_node(similar_word, label=similar_word, size=15, color='lightgreen', shape='circle')
            net.add_edge(keyword, similar_word, color='lightgray')
    
    # Ensure the output directory exists
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_path = os.path.join(output_dir, f'{category}_word_network.html')
    net.write_html(output_path)
    with open(output_path, 'r', encoding='utf-8') as HtmlFile:
        return HtmlFile.read()


def visualize_expanded_word_network(main_word, w2v_model):
    net = Network(notebook=False, height='600px', width='100%', bgcolor='#ffffff', font_color='black', layout=True)
    
    # Add main node for the selected word
    net.add_node(main_word, label=main_word, size=30, color='red', physics=False)
    
    # Find similar words and add to the network
    try:
        similar_words = w2v_model.wv.most_similar(main_word, topn=10)
        for similar_word, similarity in similar_words:
            net.add_node(similar_word, label=similar_word, size=20, color='lightgreen', physics=False)
            net.add_edge(main_word, similar_word, value=similarity, color='gray', physics=False)
            
            # Find similar words of similar words (2nd level)
            similar_words_level_2 = w2v_model.wv.most_similar(similar_word, topn=5)
            for sub_word, sub_similarity in similar_words_level_2:
                net.add_node(sub_word, label=sub_word, size=10, color='lightyellow', physics=False)
                net.add_edge(similar_word, sub_word, value=sub_similarity, color='lightgray', physics=False)
    except KeyError:
        # Skip if the keyword is not in the vocabulary
        pass
    
    # Ensure the output directory exists
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_path = os.path.join(output_dir, f'{main_word}_expanded_network.html')
    net.write_html(output_path)
    with open(output_path, 'r', encoding='utf-8') as HtmlFile:
        return HtmlFile.read()

# Load or create dataframe
@st.cache_data
def data_load(target_date):
    # ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ ìƒì„±
    engine = create_engine(db_url)
    
    # SQL ì¿¼ë¦¬ ìƒì„±
    sql = f'''
    SELECT * 
    FROM social_data 
    WHERE url IS NOT NULL 
    AND DATE(date) = '{target_date.strftime('%Y-%m-%d')}'
    '''

    sql = text(sql)
    
    # ë°ì´í„° ë¡œë“œ ë° ë‚ ì§œ í˜•ì‹ ë³€í™˜
    with engine.connect() as conn:
        df = pd.read_sql(sql, conn)
    df['date'] = pd.to_datetime(df['date'], errors='coerce')

    # contentì™€ sentencesì˜ '-' ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
    df['content'] = df['content'].replace('-', "['-']")
    df['sentences'] = df['sentences'].replace('-', "['-']")

    # contentì™€ sentencesë¥¼ ë¦¬ìŠ¤íŠ¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    df['content'] = df['content'].apply(ast.literal_eval)
    df['sentences'] = df['sentences'].apply(ast.literal_eval)
    
    return df

import streamlit as st
import pandas as pd
import altair as alt
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from kiwipiepy import Kiwi
import streamlit.components.v1 as components
import os

import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from prophet import Prophet
import streamlit as st

def stock_prediction_dashboard():
    st.title('ì£¼ê°€ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ ğŸ“ˆ')
    st.write("ì´ í˜ì´ì§€ì—ì„œëŠ” ì£¼ì‹ ì½”ë“œì™€ ì˜ˆì¸¡ ê¸°ê°„ì„ ì…ë ¥í•˜ì—¬ í•´ë‹¹ ì£¼ì‹ì˜ í–¥í›„ ê°€ê²©ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")

    # ì£¼ì‹ ì½”ë“œ ì…ë ¥ í¼
    with st.form(key='stock_form'):
        st.markdown("### ì£¼ì‹ ì½”ë“œì™€ ì˜ˆì¸¡ ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš”:")
        stock_symbol = st.text_input('ì£¼ì‹ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: TSLA, AAPL ë“±)', value='TSLA')
        prediction_period = st.number_input('ì˜ˆì¸¡í•  ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš” (ì¼ ë‹¨ìœ„, ìµœëŒ€ 30ì¼)', min_value=1, max_value=30, value=10)
        submit_button = st.form_submit_button(label='ì˜ˆì¸¡í•˜ê¸°')

    if submit_button:
        try:
            # ì£¼ì‹ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ìµœê·¼ 3ë…„, 1ì¼ ë‹¨ìœ„)
            stock = yf.Ticker(stock_symbol)
            data = stock.history(period="5y", interval="1d")  # 1ì¼ ë‹¨ìœ„ë¡œ ìµœê·¼ 3ë…„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

            # ë°ì´í„°ê°€ ë¹„ì–´ ìˆì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
            if data.empty:
                st.error("ì£¼ì‹ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì£¼ì‹ ì½”ë“œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return

            # Prophetì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ í˜•ì‹ ë³€í™˜ (íƒ€ì„ì¡´ ì œê±°)
            prophet_data = data.reset_index()[['Date', 'Close']]
            prophet_data['Date'] = prophet_data['Date'].dt.tz_localize(None)  # íƒ€ì„ì¡´ ì œê±°
            prophet_data.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)

            # Prophet ëª¨ë¸ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
            model = Prophet(
                yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=False,
                changepoint_prior_scale=0.05  # ë³€ë™ì  ë¯¼ê°ë„ ì¡°ì •
            )
            model.add_seasonality(name='monthly', period=30.5, fourier_order=5)  # ì›”ë³„ ê³„ì ˆì„± ì¶”ê°€
            model.fit(prophet_data)

            # ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            future = model.make_future_dataframe(periods=prediction_period)
            forecast = model.predict(future)

            # ìŒìˆ˜ ì˜ˆì¸¡ ê°’ì„ 0ìœ¼ë¡œ ë³€í™˜
            forecast['yhat'] = forecast['yhat'].apply(lambda x: max(x, 0))

            # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
            st.subheader('ì˜ˆì¸¡ ê²°ê³¼ ê·¸ë˜í”„')
            plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows í™˜ê²½ì—ì„œ í•œê¸€ í°íŠ¸ ì„¤ì •
            plt.figure(figsize=(12, 6))
            plt.plot(prophet_data['ds'], prophet_data['y'], label='ì‹¤ì œ ê°€ê²©', color='blue')
            plt.plot(forecast['ds'], forecast['yhat'], label='ì˜ˆì¸¡ ê°€ê²©', color='red')
            plt.xlabel('ë‚ ì§œ')
            plt.ylabel('ê°€ê²©')
            plt.legend()
            plt.grid(True, linestyle='--', alpha=0.6)
            st.pyplot(plt)

            # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ (ì˜ˆì¸¡í•œ ê¸°ê°„ë§Œ)
            st.subheader('ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°')
            future_predictions = forecast[['ds', 'yhat']].tail(prediction_period)
            future_predictions.columns = ['ë‚ ì§œ', 'ì˜ˆì¸¡ ê°€ê²©']
            st.dataframe(future_predictions)

            # ìƒì„¸ ê²°ê³¼ ê°œë³„ í‘œì‹œ
            st.markdown("### ì˜ˆì¸¡ëœ ê°€ê²© ìƒì„¸ ë³´ê¸°:")
            for _, row in future_predictions.iterrows():
                st.write(f"- ë‚ ì§œ: {row['ë‚ ì§œ']}, ì˜ˆì¸¡ ê°€ê²©: {row['ì˜ˆì¸¡ ê°€ê²©']:.2f}")

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def main():
    # í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
    # st.set_page_config(layout='wide', page_title='ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ëŒ€ì‹œë³´ë“œ', page_icon='ğŸ“Š')
    st.title('ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ëŒ€ì‹œë³´ë“œ ğŸ“Š')

    # ì‚¬ì´ë“œë°” ë° í˜ì´ì§€ ì œëª©
    st.sidebar.title('ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸')
    st.sidebar.subheader("ë°ì´í„° ì„ íƒ")
    selected_date = st.sidebar.date_input('ë‚ ì§œ ì„ íƒ', pd.Timestamp('today'))

    # ì‚¬ì´ë“œë°” - íŠ¹ì • ë‹¨ì–´ í•„í„°ë§ ê¸°ëŠ¥
    st.sidebar.subheader("ğŸ” íŠ¹ì • ë‹¨ì–´ë¡œ ê¸°ì‚¬ í•„í„°ë§")
    filter_keywords = st.sidebar.text_area("ê²€ìƒ‰í•  ë‹¨ì–´ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„):")
    filter_keywords = [word.strip() for word in filter_keywords.split(',') if word.strip()]  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    reset_filter = st.sidebar.button("ğŸ”„ í•„í„° ì´ˆê¸°í™”")

    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    if selected_date:
        df = data_load(selected_date)
        if df.empty:
            st.warning("ì„ íƒí•œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # í•„í„° ì ìš©
            if filter_keywords:
                df = df[df['content'].apply(lambda x: any(keyword in ' '.join(x) for keyword in filter_keywords))]

            if reset_filter:
                filter_keywords = []  # í•„í„° ì´ˆê¸°í™”

            st.title(f"ğŸ“ {selected_date.strftime('%Yë…„ %mì›” %dì¼')} ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸")
            # íƒ­ êµ¬ì¡°ë¡œ ë‰´ìŠ¤ ì„¸ë¶€ ì •ë³´ í‘œì‹œ (íƒ­ì„ ìƒë‹¨ì— ë°°ì¹˜)
            tab_labels = ['ë©”ì¸', 'ì •ì¹˜', 'ê²½ì œ', 'ì‚¬íšŒ', 'ìƒí™œ/ë¬¸í™”', 'IT/ê³¼í•™']
            tabs = st.tabs(tab_labels)

            # ì‚¬ì´ë“œë°” - ì‹¤ì‹œê°„ WORDCOUNT TOP 10 ë‹¨ì–´
            st.sidebar.subheader("ğŸ”¥ ì‹¤ì‹œê°„ ì¸ê¸° ë‹¨ì–´ TOP 10")
            all_tokens = []
            kiwi = Kiwi()
            for sublist in df['sentences']:
                for sentence in sublist:
                    for word in sentence:
                        analyzed = kiwi.analyze(word)
                        if analyzed:
                            morphs = analyzed[0][0]
                            for token in morphs:
                                if token.tag.startswith('N') and len(token.form) > 1:
                                    all_tokens.append(token.form)
            word_count = Counter(all_tokens)
            word_count_df = pd.DataFrame(word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
            for i, (index, row) in enumerate(word_count_df.iterrows()):
                st.sidebar.markdown(f"**{i + 1}. {row['Word']}**")

            # ë©”ì¸ í™”ë©´ ë ˆì´ì•„ì›ƒ - ì»¬ëŸ¼ ì‚¬ìš©ìœ¼ë¡œ ê°€ë…ì„± ê°œì„ 
            with tabs[0]:
                main_container = st.container()
                with main_container:
                    st.markdown("---")
                    summary_col, chart_col = st.columns([2, 1], gap="medium")

                    # ì£¼ìš” ë‰´ìŠ¤ ìš”ì•½ ì •ë³´ (ë¶„ì•¼ë³„ë¡œ 5ê°œì”©ë§Œ ë³´ì´ë„ë¡ ìˆ˜ì •)
                    with summary_col:
                        st.subheader('ğŸ“° ì£¼ìš” ë‰´ìŠ¤')
                        categories = df['category'].unique()
                        for category in categories:
                            st.markdown(f"### ğŸŒ {category} ë‰´ìŠ¤")
                            category_news = df[df['category'] == category].tail(5)
                            for i, (index, row) in enumerate(category_news.iterrows()):
                                st.markdown(f"<div style='margin-bottom: 10px;'><strong>{i + 1}. <a href='{row['url']}' target='_blank'>{row['title']}</a></strong> ğŸŒ {row['publisher']}</div>", unsafe_allow_html=True)

                    # ë¶„ì•¼ë³„ ë‰´ìŠ¤ ê°œìˆ˜ ë° ê¸/ë¶€ì • ë¹„ìœ¨ ì‹œê°í™”
                    with chart_col:
                        st.subheader('ğŸ“Š ë¶„ì•¼ë³„ ë‰´ìŠ¤ ê°œìˆ˜')
                        news_count_by_category = df['category'].value_counts()
                        news_count_df = pd.DataFrame({'Category': news_count_by_category.index, 'Count': news_count_by_category.values})
                        category_chart = alt.Chart(news_count_df).mark_bar(color='steelblue').encode(
                            x=alt.X('Count', sort='-y'),
                            y=alt.Y('Category', sort='-x', axis=alt.Axis(labelFontSize=12)),
                            tooltip=['Category', 'Count']
                        ).properties(height=300)
                        st.altair_chart(category_chart, use_container_width=True)

                        # ë¶„ì•¼ë³„ ê¸/ë¶€ì • ë¹„ìœ¨ ì‹œê°í™”
                        st.subheader('ğŸ“Š ë¶„ì•¼ë³„ ê¸ì •/ë¶€ì • ë¹„ìœ¨')
                        df['sentiment'] = df['content'].apply(lambda x: analyze_sentiment(' '.join(x)))
                        sentiment_category_df = df.groupby(['category', 'sentiment']).size().reset_index(name='count')
                        sentiment_chart = alt.Chart(sentiment_category_df).mark_bar().encode(
                            x=alt.X('count', title='Count'),
                            y=alt.Y('category', title='Category', sort='-x'),
                            color='sentiment',
                            tooltip=['category', 'sentiment', 'count']
                        ).properties(height=300)
                        st.altair_chart(sentiment_chart, use_container_width=True)

            # ì¹´í…Œê³ ë¦¬ë³„ íƒ­ êµ¬ì„±
            for idx, selected_category in enumerate(tab_labels[1:]):
                with tabs[idx + 1]:
                    filtered_data = df[df['category'] == selected_category]
                    if filtered_data.empty:
                        st.warning('í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
                        continue

                    st.subheader(f"ğŸ“ {selected_category} ë‰´ìŠ¤ ë¶„ì„")
                    sentences = [sentence for sublist in filtered_data['sentences'] for sentence in sublist]
                    w2v_model = train_word2vec_model(sentences)

                    # ì¹´í…Œê³ ë¦¬ ìƒì„¸ ë‰´ìŠ¤ ì‹œê°í™” ë ˆì´ì•„ì›ƒ
                    st.markdown("---")
                    st.subheader('ğŸ’­ ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´ ë° ë„¤íŠ¸ì›Œí¬ ë¶„ì„')
                    cloud_network_col1, cloud_network_col2 = st.columns([1, 1], gap="large")

                    # ì›Œë“œ í´ë¼ìš°ë“œ ë° ì£¼ìš” ë‹¨ì–´ ë¶„ì„
                    with cloud_network_col1:
                        st.subheader('ğŸ” ì›Œë“œ í´ë¼ìš°ë“œ')
                        tokens = []
                        for sublist in filtered_data['sentences']:
                            for sentence in sublist:
                                for word in sentence:
                                    analyzed = kiwi.analyze(word)

                                    if analyzed:
                                        morphs = analyzed[0][0]

                                    for token in morphs:
                                        if token.tag.startswith('N') and len(token.form) > 1 and token.form not in stopwords:
                                            tokens.append(token.form)
                        all_text = ' '.join(tokens)
                        wordcloud_fig = create_wordcloud(all_text)
                        st.pyplot(wordcloud_fig)

                        # ì£¼ìš” ë‹¨ì–´ ë¹ˆë„ í…Œì´ë¸”
                        word_count = Counter(tokens)
                        word_count_df = pd.DataFrame(word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
                        st.table(word_count_df)

                    # ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
                    with cloud_network_col2:
                        st.subheader('ğŸŒ ë‹¨ì–´ ë„¤íŠ¸ì›Œí¬')
                        for word in word_count_df['Word']:
                            with st.expander(f"ğŸ› ï¸ {word} ìœ ì‚¬ ë‹¨ì–´ ë„¤íŠ¸ì›Œí¬ ë³´ê¸°"):
                                expanded_network_html = visualize_expanded_word_network(word.replace('/', '_'), w2v_model)
                                components.html(expanded_network_html, height=500)

                    # ê¸ì •, ë¶€ì • í‰ê°€ ì‹œê°í™” ë° ë‰´ìŠ¤ ì˜ˆì‹œ
                    st.markdown("---")
                    st.subheader('ğŸ—³ï¸ ê¸ì •, ë¶€ì • í‰ê°€ ë¹„ìœ¨ ë° ë‰´ìŠ¤')
                    pos_neg_col1, pos_neg_col2 = st.columns([1, 1], gap="large")

                    # ê¸ì •, ë¶€ì • í‰ê°€ ì‹œê°í™”
                    with pos_neg_col1:
                        st.subheader('ğŸ“Š ê¸/ë¶€ì • ë¹„ìœ¨')
                        sentiments = filtered_data['content'].apply(lambda x: analyze_sentiment(' '.join(x)))
                        filtered_data['sentiment'] = sentiments
                        sentiment_counts = sentiments.value_counts().to_dict()
                        sentiment_df = pd.DataFrame(list(sentiment_counts.items()), columns=['Sentiment', 'Count'])
                        pie_chart = alt.Chart(sentiment_df).mark_arc(innerRadius=50).encode(
                            theta=alt.Theta('Count', stack=True),
                            color=alt.Color('Sentiment', scale=alt.Scale(scheme='category10')),
                            tooltip=['Sentiment', 'Count']
                        ).properties(height=300)
                        st.altair_chart(pie_chart, use_container_width=True)

                    # ê¸ì •, ë¶€ì • ë‰´ìŠ¤
                    with pos_neg_col2:
                        st.subheader('âœ… ê¸ì • ë‰´ìŠ¤ TOP 5')
                        positive_data = filtered_data[filtered_data['sentiment'] == 'ê¸ì •'].tail(5)
                        for i, (index, row) in enumerate(positive_data.iterrows()):
                            st.markdown(f"<div style='margin-bottom: 10px;'><strong>{i + 1}. <a href='{row['url']}' target='_blank'>{row['title']}</a></strong> ğŸŒ {row['publisher']}</div>", unsafe_allow_html=True)

                        st.subheader('âŒ ë¶€ì • ë‰´ìŠ¤ TOP 5')
                        negative_data = filtered_data[filtered_data['sentiment'] == 'ë¶€ì •'].tail(5)
                        for i, (index, row) in enumerate(negative_data.iterrows()):
                            st.markdown(f"<div style='margin-bottom: 10px;'><strong>{i + 1}. <a href='{row['url']}' target='_blank'>{row['title']}</a></strong> ğŸŒ {row['publisher']}</div>", unsafe_allow_html=True)

                    # ê¸ì •, ë¶€ì • ë‰´ìŠ¤ì˜ ì£¼ìš” ë‹¨ì–´ ë¶„ì„
                    st.markdown("---")
                    st.subheader('ğŸ’¬ ê¸ì • ë° ë¶€ì • ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´')
                    pos_neg_word_col1, pos_neg_word_col2 = st.columns([1, 1], gap="large")

                    with pos_neg_word_col1:
                        st.subheader('ğŸ’¬ ê¸ì • ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´')
                        positive_tokens = []
                        for sublist in positive_data['sentences']:
                            for sentence in sublist:
                                for word in sentence:
                                    analyzed = kiwi.analyze(word)
                                    if analyzed:
                                        morphs = analyzed[0][0]
                                        for token in morphs:
                                            if token.tag.startswith('N') and len(token.form) > 1 and token.form not in stopwords:
                                                positive_tokens.append(token.form)
                        positive_word_count = Counter(positive_tokens)
                        positive_word_count_df = pd.DataFrame(positive_word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
                        st.table(positive_word_count_df)

                    with pos_neg_word_col2:
                        st.subheader('ğŸ’¬ ë¶€ì • ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´')
                        negative_tokens = []
                        for sublist in negative_data['sentences']:
                            for sentence in sublist:
                                for word in sentence:
                                    analyzed = kiwi.analyze(word)
                                    if analyzed:
                                        morphs = analyzed[0][0]
                                        for token in morphs:
                                            if token.tag.startswith('N') and len(token.form) > 1 and token.form not in stopwords:
                                                negative_tokens.append(token.form)
                        negative_word_count = Counter(negative_tokens)
                        negative_word_count_df = pd.DataFrame(negative_word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
                        st.table(negative_word_count_df)

                    st.markdown("---")
                    st.subheader(f'ğŸ›ˆ {selected_category} ì¤‘ì‹¬ ì›Œë“œ ë„¤íŠ¸ì›Œí¬')
                    if len(tokens) > 1:
                        top_keywords = word_count_df['Word'].tolist()
                        word_network_html = visualize_main_word_network(selected_category.replace('/', '_'), top_keywords, w2v_model)
                        components.html(word_network_html, height=500)
                    else:
                        st.warning('ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')

if __name__ == "__main__":
    # main()

    st.set_page_config(layout='wide', page_title='ì¢…í•© ëŒ€ì‹œë³´ë“œ', page_icon='ğŸ“Š')
    st.sidebar.title('ğŸ“Š ëŒ€ì‹œë³´ë“œ ë©”ë‰´')
    page = st.sidebar.radio("ì´ë™í•  í˜ì´ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”:", ('ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸', 'ì£¼ê°€ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ'))

    if page == 'ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸':
        main()
    elif page == 'ì£¼ê°€ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ':
        stock_prediction_dashboard()
