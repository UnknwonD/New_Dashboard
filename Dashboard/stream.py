import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
from pyvis.network import Network
import altair as alt
import os
from kiwipiepy import Kiwi
from textblob import TextBlob
import streamlit.components.v1 as components
import ast
from sqlalchemy import create_engine
from api import db_url
from gensim.models import Word2Vec

@st.cache_data
def create_wordcloud(text):
    # Use font_path to correctly display Korean text, words are horizontal only
    wordcloud = WordCloud(width=600, height=600, background_color='white', font_path='malgun.ttf', prefer_horizontal=1.0).generate(text)
    fig, ax = plt.subplots()
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    return fig

@st.cache_data
def train_word2vec_model(sentences):
    # Train a Word2Vec model
    model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)
    return model

def visualize_main_word_network(category, top_keywords, w2v_model):
    net = Network(notebook=False, height='600px', width='100%', bgcolor='#ffffff', font_color='black', layout=True)
    
    # Add main node for the category
    net.add_node(category, label=category, size=30, color='red', physics=False)
    
    # Add nodes for top keywords and connect them to the category node
    for keyword in top_keywords:
        net.add_node(keyword, label=keyword, size=20, color='lightblue', physics=False)
        net.add_edge(category, keyword, color='gray', physics=False)
    
    # Ensure the output directory exists
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_path = os.path.join(output_dir, f'{category}_word_network.html')
    net.write_html(output_path)
    with open(output_path, 'r', encoding='utf-8') as HtmlFile:
        return HtmlFile.read()

def visualize_expanded_word_network(main_word, w2v_model):
    net = Network(notebook=False, height='600px', width='100%', bgcolor='#ffffff', font_color='black', layout=True)
    
    # Add main node for the selected word
    net.add_node(main_word, label=main_word, size=30, color='red', physics=False)
    
    # Find similar words and add to the network
    try:
        similar_words = w2v_model.wv.most_similar(main_word, topn=10)
        for similar_word, similarity in similar_words:
            net.add_node(similar_word, label=similar_word, size=20, color='lightgreen', physics=False)
            net.add_edge(main_word, similar_word, value=similarity, color='gray', physics=False)
            
            # Find similar words of similar words (2nd level)
            similar_words_level_2 = w2v_model.wv.most_similar(similar_word, topn=5)
            for sub_word, sub_similarity in similar_words_level_2:
                net.add_node(sub_word, label=sub_word, size=10, color='lightyellow', physics=False)
                net.add_edge(similar_word, sub_word, value=sub_similarity, color='lightgray', physics=False)
    except KeyError:
        # Skip if the keyword is not in the vocabulary
        pass
    
    # Ensure the output directory exists
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_path = os.path.join(output_dir, f'{main_word}_expanded_network.html')
    net.write_html(output_path)
    with open(output_path, 'r', encoding='utf-8') as HtmlFile:
        return HtmlFile.read()

# Load or create dataframe
@st.cache_data
def data_load(target_date):
    # ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ ìƒì„±
    engine = create_engine(db_url)
    
    # SQL ì¿¼ë¦¬ ìƒì„±
    sql = f'''
    SELECT * 
    FROM social_data 
    WHERE url IS NOT NULL 
    AND DATE(date) = '{target_date.strftime('%Y-%m-%d')}'
    '''
    
    # ë°ì´í„° ë¡œë“œ ë° ë‚ ì§œ í˜•ì‹ ë³€í™˜
    df = pd.read_sql(sql, engine)
    df['date'] = pd.to_datetime(df['date'], errors='coerce')

    # contentì™€ sentencesì˜ '-' ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
    df['content'] = df['content'].replace('-', "['-']")
    df['sentences'] = df['sentences'].replace('-', "['-']")

    # contentì™€ sentencesë¥¼ ë¦¬ìŠ¤íŠ¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    df['content'] = df['content'].apply(ast.literal_eval)
    df['sentences'] = df['sentences'].apply(ast.literal_eval)
    
    return df

# Sentiment analysis function
def analyze_sentiment(text):
    analysis = TextBlob(text)
    if analysis.sentiment.polarity > 0:
        return 'ê¸ì •'
    elif analysis.sentiment.polarity == 0:
        return 'ì¤‘ë¦½'
    else:
        return 'ë¶€ì •'

def main():
    st.set_page_config(layout='wide', page_title='ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ëŒ€ì‹œë³´ë“œ', page_icon='ğŸ“Š')
    st.title('ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ëŒ€ì‹œë³´ë“œ')

    # ë‚ ì§œ ì„ íƒ ì°½
    st.sidebar.subheader("ë°ì´í„° ì„ íƒ")
    selected_date = st.sidebar.date_input('ë‚ ì§œ ì„ íƒ', pd.Timestamp('today'))

    if selected_date:
        df = data_load(selected_date)
        
        if df.empty:
            st.warning("ì„ íƒí•œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ë©”ì¸ í™”ë©´ì— ë°ì¼ë¦¬ ë¦¬í¬íŠ¸ í‘œì‹œ
            st.header(f"{selected_date.strftime('%Yë…„ %mì›” %dì¼')} ë°ì¼ë¦¬ ë‰´ìŠ¤ ë¦¬í¬íŠ¸")

            # íƒ­ì„ ì´ìš©í•œ ë©”ì¸ ë° ì¹´í…Œê³ ë¦¬ ì„ íƒ
            tab_labels = ['ë©”ì¸', 'ì •ì¹˜', 'ê²½ì œ', 'ì‚¬íšŒ', 'ìƒí™œ/ë¬¸í™”', 'IT/ê³¼í•™']
            tabs = st.tabs(tab_labels)

            # ë©”ì¸ íƒ­
            with tabs[0]:
                st.header('ì „ì²´ ë‰´ìŠ¤ ìš”ì•½ ì •ë³´')
                # ì£¼ìš” ë‰´ìŠ¤
                st.subheader('ì£¼ìš” ë‰´ìŠ¤')
                for i, row in df.iterrows():
                    st.markdown(f"{i+1}. [{row['title']}]({row['url']}) / {row['category']} ë‰´ìŠ¤")
                
                # ë¶„ì•¼ë³„ ê¸ˆì¼ ë‰´ìŠ¤ ê°œìˆ˜ ì •ë¦¬
                st.subheader('ë¶„ì•¼ë³„ ë‰´ìŠ¤ ê°œìˆ˜')
                news_count_by_category = df['category'].value_counts()
                news_count_df = pd.DataFrame({'Category': news_count_by_category.index, 'Count': news_count_by_category.values})
                category_chart = alt.Chart(news_count_df).mark_bar(color='steelblue').encode(
                    x=alt.X('Category', sort='-y', axis=alt.Axis(labelAngle=-45)),
                    y='Count'
                )
                try:
                    st.altair_chart(category_chart, use_container_width=True)
                except ValueError as e:
                    st.error(f"ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                
                # ë¶„ì•¼ë³„ ì„±í–¥ ë¹„ìœ¨ ì‹œê°í™”
                st.subheader('ë¶„ì•¼ë³„ ì„±í–¥ ë¶„ì„')
                df['sentiment'] = df['content'].apply(lambda x: analyze_sentiment(' '.join(x)))
                sentiment_by_category = df.groupby('category')['sentiment'].value_counts().unstack().fillna(0)
                
                # ê° ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ íŒŒì´ì°¨íŠ¸ ê·¸ë¦¬ê¸°
                for category in sentiment_by_category.index:
                    st.markdown(f"**{category}** ë‰´ìŠ¤ ì„±í–¥ ë¶„ì„")
                    category_sentiments = sentiment_by_category.loc[category]
                    sentiment_df = pd.DataFrame({'Sentiment': category_sentiments.index, 'Count': category_sentiments.values})
                    pie_chart = alt.Chart(sentiment_df).mark_arc(innerRadius=50).encode(
                        theta=alt.Theta('Count', stack=True),
                        color=alt.Color('Sentiment', scale=alt.Scale(scheme='category10')),
                        tooltip=['Sentiment', 'Count']
                    ).properties(width=300, height=300)
                    try:
                        st.altair_chart(pie_chart, use_container_width=True)
                    except ValueError as e:
                        st.error(f"íŒŒì´ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

            # ì¹´í…Œê³ ë¦¬ë³„ íƒ­
            for idx, selected_category in enumerate(tab_labels[1:]):
                with tabs[idx + 1]:
                    # ì„ íƒí•œ ë‚ ì§œ ë° ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                    filtered_data = df[df['category'] == selected_category]
                    if filtered_data.empty:
                        st.warning('í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
                        continue

                    # Train Word2Vec model
                    sentences = [sentence for sublist in filtered_data['sentences'] for sentence in sublist]
                    w2v_model = train_word2vec_model(sentences)

                    # ì»¨í…Œì´ë„ˆ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”
                    with st.container():
                        st.markdown("---")
                        col1, col2 = st.columns([1, 1], gap="large")
                        # ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´ ì‹œê°í™” ë° ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
                        with col1:
                            st.subheader('ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´ | ì›Œë“œ í´ë¼ìš°ë“œ')
                            tokens = []
                            kiwi = Kiwi()
                            for sublist in filtered_data['sentences']:
                                for sentence in sublist:
                                    for word in sentence:
                                        analyzed = kiwi.analyze(word)

                                        if analyzed:
                                            morphs = analyzed[0][0]
                                            for token in morphs:
                                                if token.tag.startswith('N') and len(token.form) > 1:
                                                    tokens.append(token.form)
                            all_text = ' '.join(tokens)
                            wordcloud_fig = create_wordcloud(all_text)
                            st.pyplot(wordcloud_fig)
                            
                            # ì›Œë“œ ì¹´ìš´íŠ¸ ì‹œê°í™”
                            st.subheader('ê°€ì¥ ë§ì´ ë°œìƒí•œ ë‹¨ì–´')
                            word_count = Counter(tokens)
                            word_count_df = pd.DataFrame(word_count.items(), columns=['Word', 'Count']).sort_values(by='Count', ascending=False).head(10)
                            # ì´ì˜ê²Œ ê¾¸ë¯¼ í‘œë¡œ ë‹¨ì–´ ì¶œë ¥
                            st.markdown("<style>table {width: 100%; text-align: left;} th, td {padding: 8px; text-align: left; border-bottom: 1px solid #ddd;} tr:hover {background-color: #f5f5f5;}</style>", unsafe_allow_html=True)
                            st.markdown(word_count_df.to_html(index=False), unsafe_allow_html=True)

                        # ê´€ë ¨ ë²„íŠ¼ ë° ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
                        with col2:
                            st.subheader('ê´€ë ¨ ë²„íŠ¼ | ì›Œë“œ ë„¤íŠ¸ì›Œí¬')
                            # ë‹¨ì–´ í´ë¦­ ì‹œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
                            for word in word_count_df['Word']:
                                with st.expander(f"{word} ìœ ì‚¬ ë‹¨ì–´ ë„¤íŠ¸ì›Œí¬ ë³´ê¸°"):
                                    expanded_network_html = visualize_expanded_word_network(word.replace('/', '_'), w2v_model)
                                    components.html(expanded_network_html, height=750)

                            # ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
                            st.subheader('ì¹´í…Œê³ ë¦¬ ì¤‘ì‹¬ ì›Œë“œ ë„¤íŠ¸ì›Œí¬')
                            if len(tokens) > 1:
                                top_keywords = word_count_df['Word'].tolist()
                                word_network_html = visualize_main_word_network(selected_category.replace('/', '_'), top_keywords, w2v_model)
                                components.html(word_network_html, height=750)
                            else:
                                st.warning('ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')

                    # ì£¼ìš” í‚¤ì›Œë“œ ì‹œê°í™”
                    st.markdown("---")
                    st.subheader('ì£¼ìš” í‚¤ì›Œë“œ')
                    top_keywords = word_count_df['Word'].tolist()
                    st.write(f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(top_keywords)}")

                    # ë‰´ìŠ¤ ìš”ì•½ ì¶œë ¥
                    st.subheader('ë‰´ìŠ¤ ìš”ì•½ (ì£¼ìš” í‚¤ì›Œë“œ ê´€ë ¨ ê¸°ì‚¬)')
                    for keyword in top_keywords:
                        st.write(f"### í‚¤ì›Œë“œ: {keyword}")
                        keyword_articles = filtered_data[filtered_data['content'].apply(lambda x: any(keyword in sentence for sentence in x))].head(5)
                        for i, row in keyword_articles.iterrows():
                            st.markdown(f"<div style='padding: 10px; border: 1px solid #ddd; border-radius: 5px; margin-bottom: 5px;'> <a href='{row['url']}' target='_blank' style='text-decoration: none; color: #2a9d8f;'> <strong>{row['title']}</strong></a></div>", unsafe_allow_html=True)

                    # ê¸ì •, ë¶€ì • í‰ê°€ ì‹œê°í™”
                    st.subheader('ê¸ì •, ë¶€ì • í‰ê°€')
                    sentiments = filtered_data['content'].apply(lambda x: analyze_sentiment(' '.join(x)))
                    sentiment_counts = sentiments.value_counts().to_dict()
                    sentiment_df = pd.DataFrame(list(sentiment_counts.items()), columns=['Sentiment', 'Count'])
                    pie_chart = alt.Chart(sentiment_df).mark_arc(innerRadius=50).encode(
                        theta=alt.Theta('Count', stack=True),
                        color=alt.Color('Sentiment', scale=alt.Scale(scheme='category10'))
                    )
                    try:
                        st.altair_chart(pie_chart, use_container_width=True)
                    except ValueError as e:
                        st.error(f"íŒŒì´ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


if __name__ == "__main__":
    main()
